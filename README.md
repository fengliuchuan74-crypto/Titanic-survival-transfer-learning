# Titanic-survival-transfer-learning
ä¸€ä¸ªåŸºäºæ•°æ®æ¸…æ´—å’ŒPCAç‰¹å¾å·¥ç¨‹ã€ç¥ç»ç½‘ç»œçš„æ³°å¦å°¼å…‹å·å¹¸å­˜è€…é¢„æµ‹

ã€Englishã€‘

ğŸš€ Project Overview
This project addresses the classic Titanic survival prediction task using PyTorch, with a specific focus on Transfer Learning. Under a constrained environment where a pre-trained feature extractor remains "frozen," I designed and optimized a custom MLP classifier to significantly enhance predictive performance.

Final Accuracy: 81.33% (Top tier for this task)

Performance Jump: From 64.67% baseline to 81.33% through iterative tuning.

ğŸ› ï¸ Technical Highlights
Data Preprocessing: Implemented Median Imputation for missing values and robust Standardization to ensure data quality.

Feature Engineering: Applied PCA (Principal Component Analysis) to reduce 8 features down to 5, retaining 80.03% of the information variance.

Transfer Learning: Frozen a pre-trained backbone (requires_grad=False) and trained only the appended classification layers.

Model Optimization: * Integrated Batch Normalization which led to a 10% drop in Loss (from 0.49 to 0.37).

Utilized Dropout (0.2) to prevent overfitting during long-term (800 epochs) training.

ğŸ“¦ Tech Stack
Core: PyTorch, Python

Libraries: Scikit-learn, Pandas, NumPy.

ã€ä¸­æ–‡ã€‘

ğŸš€ é¡¹ç›®æ¦‚è§ˆ
æœ¬é¡¹ç›®ä½¿ç”¨ PyTorch æ¡†æ¶è§£å†³äº†ç»å…¸çš„æ³°å¦å°¼å…‹å·ç”Ÿå­˜é¢„æµ‹ä»»åŠ¡ï¼Œæ ¸å¿ƒä¾§é‡äº è¿ç§»å­¦ä¹  (Transfer Learning) çš„åº”ç”¨ã€‚åœ¨â€œå†»ç»“â€é¢„è®­ç»ƒç‰¹å¾æå–å™¨ï¼ˆä¸æ”¹å˜å…¶å‚æ•°ï¼‰çš„é™åˆ¶æ¡ä»¶ä¸‹ï¼Œæˆ‘é€šè¿‡è®¾è®¡å¹¶æŒç»­ä¼˜åŒ–è‡ªå®šä¹‰çš„ MLP åˆ†ç±»å™¨ï¼Œå®ç°äº†é¢„æµ‹æ€§èƒ½çš„æ˜¾è‘—é£è·ƒã€‚

æœ€ç»ˆå‡†ç¡®ç‡ï¼š 81.33% (è¯¥ä»»åŠ¡ä¸­çš„é¡¶çº§æ°´å¹³)

æ€§èƒ½æå‡ï¼š é€šè¿‡è¿­ä»£è°ƒä¼˜ï¼Œä» 64.67% çš„åˆå§‹æ°´å¹³æå‡è‡³ 81.33%ã€‚

ğŸ› ï¸ æŠ€æœ¯äº®ç‚¹
æ•°æ®é¢„å¤„ç†ï¼š é‡‡ç”¨ ä¸­ä½æ•°å¡«å…… ç¼ºå¤±å€¼åŠ æ ‡å‡†åŒ– (Standardization) å¤„ç†ï¼Œç¡®ä¿è¾“å…¥æ•°æ®çš„é«˜è´¨é‡ã€‚

ç‰¹å¾å·¥ç¨‹ï¼š åº”ç”¨ PCA (ä¸»æˆåˆ†åˆ†æ) å°† 8 ç»´ç‰¹å¾é™è‡³ 5 ç»´ï¼Œä¿ç•™äº† 80.03% çš„æ ¸å¿ƒä¿¡æ¯æ–¹å·®ã€‚

è¿ç§»å­¦ä¹ ç­–ç•¥ï¼š ä¸¥æ ¼æ‰§è¡Œå‚æ•°å†»ç»“ (requires_grad=False)ï¼Œä»…è®­ç»ƒæ–°å¢çš„åˆ†ç±»å¤´ï¼Œå……åˆ†åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„é«˜é˜¶ç‰¹å¾æå–èƒ½åŠ›ã€‚

æ¨¡å‹ä¼˜åŒ–å®æˆ˜ï¼š

å¼•å…¥ æ‰¹å½’ä¸€åŒ– (Batch Normalization)ï¼Œä½¿ Loss ç›´æ¥ä¸‹é™çº¦ 10%ï¼ˆä» 0.49 é™è‡³ 0.37 é™„è¿‘ï¼‰ã€‚

ä½¿ç”¨ Dropout (0.2) æœ‰æ•ˆæŠ‘åˆ¶äº† 800 è½®é•¿å‘¨æœŸè®­ç»ƒä¸­çš„è¿‡æ‹Ÿåˆé£é™©ã€‚

ğŸ“¦ æŠ€æœ¯æ ˆ
æ·±åº¦å­¦ä¹ : PyTorch

æ•°æ®å¤„ç†: Scikit-learn, Pandas, NumPy
